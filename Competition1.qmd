---
title: Analytics Report Shell
author: 
  - name: Joshua, Albert
    email: joshua.albert@snhu.edu
    affiliations: 
      - name: Southern New Hampshire University
date: today
date-format: long
theme: flatly
toc: true
---

```{r global-options, include=FALSE}
library(tidyverse)
library(tidymodels)
library(patchwork)
library(kableExtra)
library(ggridges)
library(marginaleffects)
library(reshape2)
library(knitr)
library(ggplot2)


#unregister <- function() {
  #env <- foreach :::.foreachGlobals
  #rm(list=ls(name=env),
     #pos=env)
#}


options(kable_styling_bootstrap_options = c("hover", "striped"))

theme_set(theme_bw(base_size = 14))
```

## Statement of Purpose
```{r}
competition <- read.csv("comp.csv")

data <- read.csv("data.csv")

```
The housing market can be a confusing place with houses having drastically different prices due to different property characteristics. This model is being made to identify the relationship between the price range of a property and its features. An accurate model would be able to output an accurate price range based on certain criteria and give buyers and sellers fair prices for what the home has to offer.


## Executive Summary
```{r}
  
```



## Introduction



## Exploratory Data Analysis


This initial step is attempting to clean the data into more usable data by changing the average school rating into a rounded number.
```{r}
school_edit <- data %>%
  mutate(avgSchoolRating = ifelse(avgSchoolRating <= 10, round(avgSchoolRating,0), round(avgSchoolRating, 1)))

```

This code chunk shows the proportion of each average-school-rating present in all of the houses and shows a histogram to visualize the data.
```{r}
school_edit %>%
  count(avgSchoolRating) %>%
  mutate(proportion = n/sum(n)) %>%
  kable() %>%
  kable_styling()


school_edit%>%
ggplot(aes(x = avgSchoolRating)) +
  geom_histogram(aes(x = avgSchoolRating), color = "black",
fill = "skyblue") + 
  scale_x_continuous(breaks = seq(min(school_edit$avgSchoolRating), max(school_edit$avgSchoolRating), by = 0.5)) +
  labs( 
    title = "Average School Rating Density",
    x = "School Rating",
    y = "Count"
  )

```


```{r}


```

This bar graph shows the number of homes in each price range. This is of course an important parameter to consider when creating a model for finding the price range 
```{r}

school_edit%>%
  ggplot() +
  geom_bar(aes(x = priceRange), color = "black", fill = "red") +
  
  labs(
    x = "Price Range",
    y = "Count"
  )
```
This box plot shows that the there may be a association between the average school rating and the price of the home. The mean for the higher priced homes are closest to the higher average school rating. 
```{r}
school_edit %>%
  mutate(priceRange = fct_reorder(priceRange, avgSchoolRating)) %>%
  ggplot() + 
  geom_boxplot(aes(x = avgSchoolRating, y = priceRange, fill = priceRange),
               show.legend = FALSE) +
  labs(x = "School Rating", y = "Price Range")


```



This is to show a parameter that may not be important to consider. A large proportion (94%) of the homes are single family homes and none of the price ranges have a proportion close to that, meaning it is most likely not a significant factor to consider for the price of the home.
```{r}

school_edit%>%
  count(homeType) %>%
  mutate(proportion = n/sum(n)) %>%
  kable() %>%
  kable_styling()


school_edit%>%
  ggplot() +
  geom_bar(aes(homeType), color = "black", fill = "blue") + 
theme_bw() +  # Use a clean theme
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
      
labs(
 title = "Home Type Density",
  x = " Home Type",
  y = "Count"
  )  

```


This last plot also shows that there may be a small correlation between the size of the lot and the price range, all of the means increase as the price increases, although very slightly.

```{r}
school_edit %>%
  mutate(priceRange = fct_reorder(priceRange, lotSizeSqFt)) %>%
  ggplot() + 
  geom_boxplot(aes(x = lotSizeSqFt, y = priceRange, fill = priceRange),
               show.legend = FALSE) +
               scale_x_log10() +
  labs(x = "Lot Size (sq/ft)", y = "Price Range")


```



#Competition Assignment 3
## Model Construction

To start building the Decision Tree model, a seed must be set and the data must be split into training and test data. The train data must also be split into cross validation folds (which also need a seed)
```{r}

set.seed(347)
data_splits <- initial_split(data, strata = priceRange)

train <- training(data_splits)
test <- testing(data_splits)

set.seed(476)
train_folds <- vfold_cv(train, v = 10, strata = priceRange)

```


After splitting the data, the typical model framework must be built. The model specification for the decision tree including the tuning feature and engine and mode calling is set to "rpart" and "classification" respectively. The next step is to build the recipe which is an important step in building the model. In this recipe, we can remove useless columns and execute code that will ensure missing data does not cause mathematical and computational errors. The last step is to create the workflow with the specification and recipe.
## Decision Tree
```{r}
dt_spec <- decision_tree(tree = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

dt_recipe <- recipe(priceRange ~., data = train) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  #step_rm(all_nominal_predictors) %>% 
  step_rm(description) %>% 
  step_rm(latitude) %>% 
  step_rm(longitude) %>% 
  step_novel(city) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_impute_median(all_numeric_predictors())
  

dt_wf <- workflow() %>% 
  add_model(dt_spec) %>% 
  add_recipe(dt_recipe)


```
In this model we chose to tune the tree() funciton, so we used "tune_grid" and our cross validation data set to fit a model that gives us the log loss mean of how our model performs. 
```{r}
#n_cores <- parallel::detectCores()
#cluster <- parallel::makeCluster(n_cores - 3, type = "PSOCK")
#doParallel::registerDoParallel(cluster)
#tictoc::tic()
dt_tune_results <- dt_wf %>%
  tune_grid(
    grid = 3,
    resamples = train_folds,
    metrics = metric_set(mn_log_loss)
  )

#parallel::stopCluster(cluster)
#tictoc::toc()
```

Below is the call of the mean of the log loss prediction. This is done by finalizing our work flow to get the best parameter and showing best results.

```{r}


dt_best_param <- dt_tune_results %>% 
  select_best(metric = "mn_log_loss")

dt_final_wf <- dt_wf %>% 
  finalize_workflow(dt_best_param)

dt_fit <- dt_final_wf %>% 
  fit(train)

dt_fit %>%
  extract_fit_engine() %>%
  rpart.plot::rpart.plot()

dt_tune_results %>% 
  show_best()

submission <- dt_fit %>%
  augment(competition) %>%
  rename(
    prob_A = ".pred_0-250000",
    prob_B = ".pred_250000-350000",
    prob_C = ".pred_350000-450000",
    prob_D = ".pred_450000-650000",
    prob_E = ".pred_650000+"
  ) %>%
  select(id, starts_with ("prob"))

write.csv(submission, "submissionDT.csv", row.names = FALSE)

#my_submission <- dt_fit %>%
 # augment(train) %>%
 # rename(kissed = .pred_yes) %>%
 # select(id, priceRange)
#write.csv(my_submission, "submissionJoshDT.csv", row.names = FALSE)
```
This Decision Tree model focuses on 3 important predictors involved in predicting the price range of the different homes. As you can see in the decision tree, it first asks about the school rating, if the condition is no, it asks if the number of bathrooms is less than 1.1. The lst quesiton involves asking if the lot size is less than 0.067. The log loss mean is 1.3 which is pretty good but may change with the data in the kaggle competition.


## K Nearest Neighbor

The K Nearest neighbor model works differently from the Decision Tree model but the specification, recipe and workflow is the same. By setting a specification engine to "kknn" and the mode to "classification" we can create a recipe the removes unimportant data and normalizes data that may create inaccurate trends in the data. Last we can add our specification and recipe to a workflow for tidy models to execute.

```{r}

knn_spec <- nearest_neighbor(neighbors = tune()) %>% 
  set_engine("kknn") %>% 
  set_mode("classification")


knn_rec <- recipe(priceRange ~., data = train) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_rm(all_nominal_predictors()) %>% 
  step_rm(latitude) %>% 
  step_rm(longitude) %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors())


knn_wf <- workflow() %>% 
  add_model(knn_spec) %>% 
  add_recipe(knn_rec)

```

By doing the same as the Decision tree model, we can extract the log loss mean for the best grid searches by finishing the workflow and tuning the neighbor parameter. The log loss metric shows a best mean of 2.6 and the worst of 4.3 but in kaggle this value changed drastically giving us a mean of 44.7.

```{r}


knn_cv_results <- knn_wf %>% 
     tune_grid (
       grid = 10,
    resamples = train_folds,
    metrics = metric_set(mn_log_loss)
  )

knn_best_param <- knn_cv_results %>% 
  select_best(metric = "mn_log_loss")

knn_final_wf <- knn_wf %>% 
  finalize_workflow(knn_best_param)

knn_fit <- knn_final_wf %>% 
  fit(train)



knn_cv_results %>% 
  show_best()

submission <- knn_fit %>%
  augment(competition) %>%
  rename(
    prob_A = ".pred_0-250000",
    prob_B = ".pred_250000-350000",
    prob_C = ".pred_350000-450000",
    prob_D = ".pred_450000-650000",
    prob_E = ".pred_650000+"
  ) %>%
  select(id, starts_with ("prob"))

write.csv(submission, "submissionKNN.csv", row.names = FALSE)

```
```{r}


```


```{r}



```

#Competition Assignment 4
## Ensembles of Data

```{r}

rf_tune_spec <- rand_forest(min_n = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

rf_tune_rec <- recipe(priceRange ~., data = train) %>% 
  step_rm(description) %>% 
  step_rm(latitude) %>% 
  step_rm(longitude) %>% 
    step_novel(city) %>% 
  step_unknown(all_nominal_predictors()) %>% 
  step_other(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_impute_median(all_numeric_predictors())

rf_tune_wf <- workflow() %>% 
  add_model(rf_tune_spec) %>% 
  add_recipe(rf_tune_rec)

```
```{r}
#n_cores <- parallel::detectCores()
#cluster <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
#doParallel::registerDoParallel(cluster)
#tictoc::tic()
#rf_tune_results <- rf_tune_wf %>%
 # tune_grid(
  #  grid = 5,
  #  resamples = train_folds,
 #   metrics = metric_set(mn_log_loss)
 # )
#parallel::stopCluster(cluster)
#tictoc::toc()

```

```{r}


rf_cv_results <- rf_tune_wf %>% 
     tune_grid (
       grid = 10,
    resamples = train_folds,
    metrics = metric_set(mn_log_loss)
  )

rf_best_param <- rf_cv_results %>% 
  select_best(metric = "mn_log_loss")

rf_final_wf <- rf_tune_wf %>% 
  finalize_workflow(rf_best_param)

rf_fit <- rf_final_wf %>% 
  fit(train)



rf_cv_results %>% 
  show_best()

submission <- rf_fit %>%
  augment(competition) %>%
  rename(
    prob_A = ".pred_0-250000",
    prob_B = ".pred_250000-350000",
    prob_C = ".pred_350000-450000",
    prob_D = ".pred_450000-650000",
    prob_E = ".pred_650000+"
  ) %>%
  select(id, starts_with ("prob"))

write.csv(submission, "submissionRF.csv", row.names = FALSE)

rf_cv_results %>% 
  show_best()



```
The random forest model did not result in a better log loss metric than the Decision Tree model. In the original model of the random forest the "trees" parameter was tuned. In the second model, "min_n" was tuned but did not yield better results.

## Boosted Tree
```{r}

boost_tree_spec <- boost_tree(trees = tune()) %>% 
set_engine("xgboost") %>% 
set_mode("classification")

boost_tree_rec <- recipe(priceRange ~., data = train) %>% 
  step_rm(description) %>%
   step_rm(latitude) %>% 
  step_rm(longitude) %>% 
    step_novel(city) %>% 
  step_normalize(all_nominal_predictors()) %>% 
  step_unknown(all_nominal_predictors()) %>% 
    step_other(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_impute_median(all_numeric_predictors())

bt_tree_wf <- workflow() %>% 
  add_model(boost_tree_spec) %>% 
  add_recipe(boost_tree_rec)

```


```{r}

bt_cv_results <- bt_tree_wf %>% 
     tune_grid (
       grid = 2,
    resamples = train_folds,
    metrics = metric_set(mn_log_loss)
  )

bt_best_param <- bt_cv_results %>% 
  show_best(metric = "mn_log_loss")

bt_final_wf <- bt_tree_wf %>% 
  finalize_workflow(bt_best_param)

bt_fit <- bt_final_wf %>% 
  fit(train)


bt_cv_results %>% 
  show_best()




```

## Model Interpretation and Inference



## Conclusion



## References
```{r}



```


